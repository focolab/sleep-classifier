#!/usr/bin/env python3
#
#
#   Sleep state classifier model training
# 
#   Training and cross validation are carried out for all combinations of 
#   subsampling parameters (i.e. training with 2h per 24h recording) and 
#   classifiers. 
#
#   Subsampling is carried out by splitting each 24h recording (8640 epochs)
#   into uniformly spaced chunks and creating a binary mask.
#
#   The four (implemented) classifier models are
#       - LDA: linear discriminatn analysis
#       - QDA: quadtratic discriminatn analysis
#       - OVO: one versus one
#       - OVR: one versus rest
#
#   Each subsampling/classifier combination is scored by leave-one-out cross
#   validation, where single trials (24h recording) are held-out.
#
#   In addition to returning raw confusion matrices, some accuracy metrics are
#   also computed and returned/dumped as dataframes
#
#   TODO: rename hpscheme? training subsampling scheme is a mouthful
#
#
#======================================
import os
import argparse
import json
import warnings
import pdb
import pickle

import pandas as pd
import numpy as np

import sklearn
from sklearn import svm
from sklearn.svm import LinearSVC
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.multiclass import OneVsOneClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import LeaveOneOut
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix

import remtools as rt
import modeltools as mt

def ppd(d):
    for k, v in d.items():
        print('%15s:' % k, v)


def get_label_accuracy(cnf=None, labels=None):
    """label accuracy is a homemade accuracy metric for each specific label

        LA = 2*TP/(2*TP+FP+FN)

        where TP, FP, and FN are counts of true positives, false positives and false
        negatives only for one label
    
        LA ranges from [0,1], zero being worst and one being perfect
    """
    lacc = {}
    for i, label in enumerate(labels):
        lacc[label] = 2*cnf[i,i]/(np.sum(cnf[:,i]) + np.sum(cnf[i,:]))
    return lacc

def get_df_lacc(models=None, labels=None, label=None, doavg=True):
    """dataframe summarizing label accuracy scores across trials and classifiers"""
    classifier_names = models[0]['classifiers'].keys()
    dd = []
    for m in models:
        tmp = []
        for c in classifier_names:
            cnf = m['classifiers'][c]['confusion']
            lacc = get_label_accuracy(cnf=cnf, labels=labels)
            tmp.append(lacc[label])
        dd.append(tmp)

    df = pd.DataFrame(data=dd, columns=classifier_names)
    df.index = [m['trial'] for m in models]
    df.index.name = 'trial (val)'
    if doavg:
        df.loc["Avg"] = df.mean()
    return df


def get_df_acc(models=None, doavg=True):
    """dataframe summarizing (canonical) accuracy across trials and classifier_names"""
    classifier_names = models[0]['classifiers'].keys()
    dd = []
    for m in models:
        tmp = []
        for c in classifier_names:
            cnf = m['classifiers'][c]['confusion']
            acc = np.trace(cnf)/np.sum(cnf)
            tmp.append(acc)
        dd.append(tmp)

    df = pd.DataFrame(data=dd, columns=classifier_names)
    df.index = [m['trial'] for m in models]
    df.index.name = 'trial (val)'
    if doavg:
        df.loc["Avg"] = df.mean()
    return df




def training_loo(X=None, y=None, df_index=None, params_train={}):
    """training with leave-one-out cross validation and training subsets/masks

    wrapper around train_models()

    arguments
    ------
    X: np.array
        features (row/col are observations/features)
    y: list/np.array
        labels
    df_index: pandas.DataFrame
        indexing information
    params_train: dictionary
        parameters that get passed through to train_models


    NOTES:
    Leave-one-out: Training data are grouped by 'trial' for leave-one-out
        cross validation
    Training masks: A subset of data on which to train can be generated by
        applying boolean masks (columns in df_index). This can be used to
        skip epochs without a (human) scoring consensus and/or to train on only
        a subset of data (such as evenly spaced chunks)

    df_index columns
    ------    
    trial: (int/str) trial names, used for leave-one-out xvalidation
    isTraining: (bool) used to train on a subset of training data

    TODO: training masks (isValid, isTraining) for X
    TODO: loo_col (for leave-one-out grouping)

    """

    trn_mask_cols = ['isTraining', 'isValid']
    val_mask_cols = ['isValid']
    loo_group_col = 'trial'


    trials = df_index['trial'].unique()
    print('trials:', trials)

    #==============================================
    # LOO MODELS -- cross validation
    loo_models = []
    splits = list(LeaveOneOut().split(trials))
    y_pred = []
    for train_index, test_index in splits:
        trial = trials[test_index[0]]

        print(df_index.head())
        # indexing
        # training and validation (trn/val) indices
        df_trn = df_index[(df_index['trial'] != trial)]
        for col in trn_mask_cols:
            df_trn = df_trn[df_trn[col]]

        df_val = df_index[df_index['trial'] == trial]
        for col in val_mask_cols:
            df_val = df_val[df_val[col]]

        ndx_trn = df_trn.index.values
        ndx_val = df_val.index.values

        # training and validation
        Xt = X[ndx_trn]
        Xv = X[ndx_val]
        yt = y[ndx_trn]
        yv = y[ndx_val]

        # numbers
        num_trn = len(ndx_trn)
        num_val = len(ndx_val)
        print(" TRAIN/TEST:", train_index, test_index, '-- num_trn/num_val: %6i/%6i' % (num_trn, num_val))

        # DO THE TRAINING
        mdic = train_models(Xt=Xt, Xv=Xv, yt=yt, yv=yv, params=params_train)
        mdic['trial'] = trial
        
        # predict ALL
        ndx_prd = df_index[df_index['trial'] == trial].index.values
        df_prd , data_cols = mdic['cb'].predict(X=X[ndx_prd])
        df_prd['trial'] = [trial]*df_prd.shape[0]
        index_cols = ['trial', 'classifier_name']
        df_prd = df_prd[index_cols+data_cols]
        mdic['predictions'] = dict(
            df=df_prd,
            index_cols=index_cols,
            data_cols=data_cols
            )

        loo_models.append(mdic)


    #==============================================
    # FULL MODEL (not leaving one out)

    # indexing
    # training and validation (trn/val) indices
    df_trn = df_index.copy()
    for col in trn_mask_cols:
        df_trn = df_trn[df_trn[col]]
    ndx_trn = df_trn.index.values

    df_val = df_index.copy()
    for col in val_mask_cols:
        df_val = df_val[df_val[col]]
    ndx_val = df_val.index.values

    # training and validation
    Xt = X[ndx_trn]
    Xv = X[ndx_val]
    yt = y[ndx_trn]
    yv = y[ndx_val]

    # numbers
    num_trn = len(ndx_trn)
    num_val = len(ndx_val)
    print(" TRAIN/TEST: ALL/ALL", '-- num_trn/num_val: %6i/%6i' % (num_trn, num_val))

    mdic = train_models(Xt=Xt, Xv=Xv, yt=yt, yv=yv, params=params_train)
    mdic['trial'] = 'full_training_dataset'
    full_model = mdic

    return loo_models, full_model


def train_models(Xt=None, yt=None, Xv=None, yv=None, params={}):
    """classifier model training and validation

    inner most routine!?

    Four classifiers are implemented:
        - LDA: linear discriminant analysis
        - QDA: quadratic discriminant analysis
        - OVO: one-vs-one
        - OVR: one-vs-rest


    arguments
    ------
    Xt/Xv: np.array
        feature vectors for training (t) and validation (v)
    yt/yv: list/np.array
        labels, corresponding to X, for training (t) and validation (v)

    params: dict
        doStandardize: bool
            standardize training features?
        numPCs: int
            PCA dimension reduction of training features (after standardization)
        quietmode: bool
            suppress sklearn noise during training
        labels: list
            names of the labels, for sorting confusion matrices
        num_lda: int
            number of dimensions for LDA classifier
        LSVCparam: dict
            LinearSVC parameters

    returns
    ------
    mdic: dictionary
        dictionary w/ transformations, trained models and confusion matrices

    """

    LSVCparam = dict(random_state=0, verbose=0, max_iter=3000)    

    # parametersss
    p = dict(
        _about='parameters for train_models()',
        doStandardize=True,
        numPCs=0,
        quietmode=True,
        labels=None,
        num_lda=2,
        LSVCparam=LSVCparam
        )
    p.update(params)

    # assign labels if not already set
    if p['labels'] is None:
        p['labels'] = np.unique('yt').tolist()

    # labeling sanity checks
    lbls_trn = np.unique(yt).tolist()
    lbls_val = np.unique(yv).tolist()

    # training and validation labels (unique) must match
    if set(lbls_trn) != set(lbls_val):
        print('LABELS MISMATCH: train/validate %s' % (str([lbls_trn, lbls_val])))
        raise Exception()

    # passed labels also must match
    for label in p['labels']:
        if label not in lbls_trn:
            print('LABEL [%s] not found in trn/val labels' % label, lbls_trn)
            raise Exception()


    #-------------------------------------------------------
    # standardization
    if p['doStandardize']:
        sc = StandardScaler().fit(Xt)
        Xt = sc.transform(Xt)
        Xv = sc.transform(Xv)
    else:
        sc = None

    # PCA
    if p['numPCs'] > 0:
        pca = PCA(n_components=numPCs).fit(Xt)
        Xt = pca.transform(Xt)
        Xv = pca.transform(Xv)
    else:
        pca = None

    # MODEL TRAINING
    lda = LinearDiscriminantAnalysis(n_components=p['num_lda']).fit(Xt, yt)
    qda = QuadraticDiscriminantAnalysis().fit(Xt, yt)
    with warnings.catch_warnings():
        if p['quietmode']:
            warnings.filterwarnings("ignore", category=sklearn.exceptions.ConvergenceWarning)
        ovo = OneVsOneClassifier(LinearSVC(**LSVCparam)).fit(Xt, yt)
        ovr = OneVsRestClassifier(LinearSVC(**LSVCparam)).fit(Xt, yt)

    # confusion matrices / validation
    cnf_lda = confusion_matrix(yv, lda.predict(Xv), labels=p['labels'])
    cnf_qda = confusion_matrix(yv, qda.predict(Xv), labels=p['labels'])
    cnf_ovr = confusion_matrix(yv, ovr.predict(Xv), labels=p['labels'])
    cnf_ovo = confusion_matrix(yv, ovo.predict(Xv), labels=p['labels'])

    cb = mt.ClassifierBundle(
        sc=sc, 
        pca=pca, 
        training_params=p,
        classifiers=dict(
            LDA=dict(cls=lda, confusion=cnf_lda),
            QDA=dict(cls=qda, confusion=cnf_qda),
            OVO=dict(cls=ovo, confusion=cnf_ovo),
            OVR=dict(cls=ovr, confusion=cnf_ovr),
            ),
        )

    # pack results into a dictionary
    mdic = dict(
        _about="classifier models made with train_models()",
        cb=cb,
        sc=sc,
        pca=pca,
        num_trn=len(yt),
        num_val=len(yv),
        classifiers=dict(
            LDA=dict(cls=lda, confusion=cnf_lda),
            QDA=dict(cls=qda, confusion=cnf_qda),
            OVO=dict(cls=ovo, confusion=cnf_ovo),
            OVR=dict(cls=ovr, confusion=cnf_ovr),
        ),
        params=p
        )
    return mdic




#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('-f', nargs='+', type=str, help='staged data json files')
    parser.add_argument('--dest', default='ANL-models', help='output folder')
    args = parser.parse_args()

    os.makedirs(args.dest, exist_ok=True)

    print('#=================================================================')
    print('           anl-trainmodels.py')
    print('#=================================================================')

    # parameterssssss
    params_train = dict(
        doStandardize=True,
        numPCs=0,
        quietmode=True,
        labels=['REM', 'Non REM', 'Wake']
        )

    scoreKey = 'cScoreStr'  # score column name in y (DataFrame)
    keep_classes = params_train.get('labels')

    # hyperparameter schemes (hpscheme) LOOP OVER
    hpschemes = [dict(tag='01h_8ch', pss=dict(num_total=8640, num_keep=360,  num_chunks=8)),
                 dict(tag='02h_8ch', pss=dict(num_total=8640, num_keep=720,  num_chunks=8)),
                 dict(tag='04h_8ch', pss=dict(num_total=8640, num_keep=1440, num_chunks=8)),
                 dict(tag='24h_8ch', pss=dict(num_total=8640, num_keep=8640, num_chunks=8)),
                 ]


    # load training data
    allTrialData = [rt.StagedTrialData.from_json(f, loadEDF=False) for f in args.f]
    Xcat = np.vstack([std.sxxb_prep.stack.T for std in allTrialData])

    print('USE SCOREBLOCK FOR CONSENSUS SCORES')
#    ycat = pd.concat([std.sw.dfConsensus for std in allTrialData]).reset_index(drop=True)
    # ycat needs to be a concatenated vector of consensus scores
    ycat = []
    for std in allTrialData:
        #std.scoreblock.about()
        yy = std.scoreblock.keeprows(conditions = [('scorer', 'consensus')])
        ycat.append(yy.data.ravel())
    ycat = np.asarray(ycat).ravel()


    # create a global index of trial, Epoch#, and isValid
    dfs = []
    for i, std in enumerate(allTrialData):
        trial = std.trial

        # y = std.sw.dfConsensus
        # keep = [x in keep_classes for x in y['cScoreStr']]
        y = std.scoreblock.keeprows(conditions = [('scorer', 'consensus')]).data.ravel()
        keep = [x in keep_classes for x in y]

        epoch = list(range(1, len(y)+1))
        data = zip([trial]*len(y), epoch, keep)
        columns = ['trial', 'Epoch#', 'isValid']
        df = pd.DataFrame(data=data, columns=columns)
        dfs.append(df)
    df_index = pd.concat(dfs).reset_index(drop=True)

    pdb.set_trace()

    # train models using different subsamples of training data
    prediction_dataframes = []
    for i, pp in enumerate(hpschemes):

        params_subsampling = pp['pss']

        tagDict = {}
        tagDict['num_training_epochs'] = params_subsampling['num_keep']
        tagDict['tag'] = pp['tag']
        tagDict['hpscheme'] = '%3.3i' % i
        tag = 'hpscheme-%3.3i' % i

        fldr = os.path.join(args.dest, 'training-%s' % tag)
        os.makedirs(fldr, exist_ok=True)
        print('#====================================================')
        print(tag)

        # set up the isTraining mask (training chunks)
        train = mt.chop8640(**params_subsampling)
        df_index['isTraining'] = train*len(allTrialData)


        # leave one out (LOO) cross validation
        xv_models, full_model = training_loo(
            X=Xcat,
            #y=ycat[scoreKey].values,
            y=ycat,
            df_index=df_index, 
            params_train=params_train
            )


        # LOO summary statistics
        df_acc = get_df_acc(models=xv_models)
        df_lacc = get_df_lacc(models=xv_models, labels=keep_classes, label='REM')


        # stack all the predictions
        p0 = xv_models[0]['predictions']
        df_pred = pd.concat([m['predictions']['df'] for m in xv_models], axis=0)
        df_pred['hpscheme'] = [tagDict['tag']]*len(df_pred)
        index_cols = ['hpscheme']+p0['index_cols']
        data_cols = p0['data_cols']
        df_pred = df_pred[index_cols+data_cols]
        xv_predictions = dict(
            df=df_pred,
            index_cols = index_cols,
            data_cols = data_cols,
        )

        print(df_pred)

        # i can has multi-index?
        df_predmi = df_pred.set_index(['trial', 'hpscheme','classifier_name'])
        prediction_dataframes.append(df_predmi)

        #======================================================================
        # dumppp output
        csv = os.path.join(fldr, 'df_index.csv')
        df_index.to_csv(csv, float_format='%g')

        csv = os.path.join(fldr, 'df_model_acc.csv')
        df_acc.to_csv(csv, float_format='%g')

        csv = os.path.join(fldr, 'df_model_lacc_REM.csv')
        df_lacc.to_csv(csv, float_format='%g')

        csv = os.path.join(fldr, 'df_xv_predictions.csv')
        xv_predictions['df'].to_csv(csv, float_format='%g')


        trdata = dict(
            loc=os.path.abspath(fldr),
            tagDict=tagDict,
            df_acc=df_acc,
            df_lacc=df_lacc,
            df_index=df_index,
            df_xv_predictions=df_predmi,
            xv_models=xv_models,
            xv_predictions=xv_predictions,
            full_model=full_model,
            params_train=params_train,
            params_subsampling=params_subsampling
            )

        pf = os.path.join(fldr, 'model.p')
        with open(pf, 'wb') as pfh:
            pickle.dump(trdata, pfh)


